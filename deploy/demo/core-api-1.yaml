---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: lh-server
  namespace: default
  labels:
    app: lh-server
    io.littlehorse/active: "true"
spec:
  replicas: 1
  serviceName: lh-server
  podManagementPolicy: Parallel
  selector:
    matchLabels:
      app: lh-server
  template:
    metadata:
      labels:
        app: lh-server
        littlehorse.io/active: "true"
    spec:
      topologySpreadConstraints:
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        whenUnsatisfiable: ScheduleAnyway
        labelSelector:
          matchLabels:
            app: lh-server
      serviceAccountName: lh-server
      containers:
      - name: lh-server
        image: 834373697194.dkr.ecr.us-west-2.amazonaws.com/littlehorse:latest
        imagePullPolicy: Always
        startupProbe:
          exec:
            command:
            - /lhctl
            - k8sHealth
          failureThreshold: 5
          periodSeconds: 3

        livenessProbe:
          exec:
            command:
            - /lhctl
            - k8sHealth
          failureThreshold: 5
          periodSeconds: 3

        readinessProbe:
          exec:
            command:
              - /lhctl
              - k8sHealth
              # TODO: If we re-enable the --readiness flag, then the healthcheck will
              # only return "OK" if state==RUNNING. This sounds good in theory, but
              # during a rebalance, all pods will be considered down even if they are
              # unaffected (i.e. cooperative rebalance). We can get around this by
              # just using the same logic as the liveness probe (return OK unless in
              # ERROR or SHUTDOWN state), and by using standby replicas for queries,
              # and also accepting write requests by pushing them to Kafka.
              # THat seems better, but to implement that, we need to actually do
              # the thing to query standby replicas.
              # - --readiness
          failureThreshold: 1
          periodSeconds: 3

        env:
        - name: LHORSE_KAFKA_BOOTSTRAP
          value: "lh-kafka-kafka-bootstrap.kafka:9092"
        - name: "LHORSE_KAFKA_GROUP_ID"
          value: "server"
        - name: "LHORSE_KAFKA_GROUP_IID"
          valueFrom:
            fieldRef:
              fieldPath: metadata.name

        - name: "LHORSE_CLUSTER_PARTITIONS"
          value: "12"
        - name: "LHORSE_REPLICATION_FACTOR"
          value: "1"
        - name: "LHORSE_NUM_STREAM_THREADS"
          value: "2"
        - name: "LHORSE_COMMIT_INTERVAL"
          value: "1000"
        - name: "LHORSE_KAFKA_STREAMS_STATE_DIR"
          value: "/kafkaStateStores"
        - name: "LHORSE_NUM_STANDBY_REPLICAS"
          value: "0"

        - name: "LHORSE_DEFAULT_TIMEOUT"
          value: "15"
        - name: "LHORSE_KAFKA_TOPIC_PREFIX"
          value: ""

        - name: LHORSE_ADVERTISED_LISTENERS
          value: "LHORSE_PUBLIC_LISTENER"
        - name: "LHORSE_PUBLIC_LISTENER"
          value: "demo-1.littlehorse.cloud:5000"

        - name: LHORSE_API_BIND_PORT
          value: "5000"
        - name: LHORSE_INTERNAL_ADVERTISED_HOST
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: LHORSE_INTERNAL_ADVERTISED_PORT
          value: "5001"
        - name: LHORSE_INTERNAL_BIND_PORT
          value: "5001"

        ports:
        - containerPort: 5000
        - containerPort: 5001
        command: ['/server']
        volumeMounts:
        - name: streams-state
          mountPath: /kafkaStateStores
  volumeClaimTemplates:
  - metadata:
      name: streams-state
      labels:
        littlehorse.io/kafka-state-dir: "true"
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: "gp2"
      resources:
        requests:
          storage: 5Gi
